{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0116a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of the Vertex AI Pipeline, serverless ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8942e7",
   "metadata": {
    "id": "mVX0RV_s8zZ7"
   },
   "source": [
    "# Vertex Pipelines: Model train, upload, and deploy using google-cloud-pipeline-components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f299f215",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794313d7",
   "metadata": {
    "id": "IaYsrh0Tc17L"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7bfb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yxtzwPPNZ-SH",
    "outputId": "92d616ad-4586-4a88-ee15-dab67a2eb152"
   },
   "outputs": [],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.0.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components==0.1.1 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383d326",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d73448",
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f1bb8",
   "metadata": {
    "id": "6GPgNN7eeX1l"
   },
   "source": [
    "Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3cf645",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NN0mULkEeb84",
    "outputId": "865fafbd-91da-42f7-f4f7-b1ec1e916493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.6.2\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e350cf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM1iC_MfAts1",
    "outputId": "58256a2d-2fb0-4b30-d450-074d25e09b9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  google.com:ml-baguette-demos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b12ff3",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121c557d",
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"google.com:ml-baguette-demos\"  \n",
    "    !gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc019d0a",
   "metadata": {
    "id": "NxhCPW6e46EF"
   },
   "source": [
    "### Create a Cloud Storage bucket as necessary\n",
    "\n",
    "You will need a Cloud Storage bucket for this example.  If you don't have one that you want to use, you can make one now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae16da1c",
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "# Change to YOUR bucket name\n",
    "BUCKET_NAME = \"gs://auv_vertex_pipeline\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33b7ceff",
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ac3e8",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d972577d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIq7R4HZCfIc",
    "outputId": "db9469a1-7a4c-485a-a471-be25d66a8721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://auv_vertex_pipeline/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'auv_vertex_pipeline' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc023608",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09f8858",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhOb7YnwClBb",
    "outputId": "01cab4b4-3e70-4a1b-8cf9-c5ce8f50db2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       575  2021-05-21T15:53:05Z  gs://auv_vertex_pipeline/trainer_cifar.tar.gz#1621612385302471  metageneration=1\n",
      "                                 gs://auv_vertex_pipeline/custom_job_20210521125951/\n",
      "                                 gs://auv_vertex_pipeline/custom_job_20210524093044/\n",
      "                                 gs://auv_vertex_pipeline/pipeline_root/\n",
      "                                 gs://auv_vertex_pipeline/staging/\n",
      "TOTAL: 1 objects, 575 bytes (575 B)\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83aa2eb",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6103bdc",
   "metadata": {
    "id": "YYtGjGG45ELJ"
   },
   "source": [
    "Define some constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f91659",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "5zmD19ryCre7",
    "outputId": "428e4a28-ee49-4d33-943a-92b97b792257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://auv_vertex_pipeline/pipeline_root/auv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "USER = \"auv\"  # @param {type:\"string\"}\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)\n",
    "\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc384a60",
   "metadata": {
    "id": "IprQaSI25oSk"
   },
   "source": [
    "Do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30302d4f",
   "metadata": {
    "id": "UFDUBveR5UfJ"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4eb71",
   "metadata": {
    "id": "accelerators:training,prediction"
   },
   "source": [
    "#### Set hardware accelerators\n",
    "You can set hardware accelerators for both training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b1bcf5a8",
   "metadata": {
    "id": "xd5PLXDTlugv"
   },
   "outputs": [],
   "source": [
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
    "\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_K80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860fab4",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "#### Set pre-built containers\n",
    "\n",
    "Vertex AI provides pre-built containers to run training and prediction.\n",
    "\n",
    "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers) and [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42726be1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1u1mr18jlugv",
    "outputId": "6ebe2573-fe13-4e2c-9cfc-31e66c3514be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: tensorflow/tensorflow:latest-gpu AcceleratorType.NVIDIA_TESLA_K80 1\n",
      "Deployment: gcr.io/cloud-aiplatform/prediction/tf2-gpu.2-4:latest AcceleratorType.NVIDIA_TESLA_K80 1\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VERSION = \"tf-gpu.2-4\"\n",
    "DEPLOY_VERSION = \"tf2-gpu.2-4\" \n",
    "\n",
    "TF2_GPU_IMAGE = \"tensorflow/tensorflow:latest-gpu\"\n",
    "TRAIN_IMAGE = \"gcr.io/cloud-aiplatform/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TF2_GPU_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0075160",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### Set machine types\n",
    "\n",
    "Next, set the machine types to use for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "661ab0c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAXwbqKKlugv",
    "outputId": "2110ed52-8458-4d2e-9926-383474b01393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6eb2bb",
   "metadata": {
    "id": "AqoS1fRn76ff"
   },
   "source": [
    "#### Instantiate an Vertex Pipeline KFP API client object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "adf1d0eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHH5mSWBvqFt",
    "outputId": "3bd793a9-f8b9-4750-9f6a-9f31124598f4"
   },
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cc731",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c76fe",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c756abf1",
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6fef9779",
   "metadata": {
    "id": "1npiDcUtlugw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://auv_vertex_pipeline/custom_job_20210524150230\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"custom_job_\" + TIMESTAMP\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_NAME, JOB_NAME)\n",
    "MODEL_DISPLAY_NAME = \"cifar10-\" + TIMESTAMP\n",
    "\n",
    "if not TRAIN_NGPU or TRAIN_NGPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"mirror\"\n",
    "\n",
    "EPOCHS = 10\n",
    "STEPS = 100\n",
    "\n",
    "print(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36ecd9",
   "metadata": {
    "id": "taskpy_contents"
   },
   "source": [
    "#### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf19afd5",
   "metadata": {
    "id": "NJK00XwJuDJD"
   },
   "outputs": [],
   "source": [
    "# Prepare a function to convert it to a component\n",
    "\n",
    "@component(base_image=TF2_GPU_IMAGE,\n",
    "           output_component_file='train_op.yaml', \n",
    "           packages_to_install=['tensorflow-datasets'])\n",
    "def train(\n",
    "  lr: float,\n",
    "  epochs: int,\n",
    "  steps: int,\n",
    "  distribute: str,\n",
    "  model_uri: str,\n",
    "):\n",
    "  \n",
    "  # Imports\n",
    "  import tensorflow_datasets as tfds\n",
    "  import tensorflow as tf\n",
    "  from tensorflow.python.client import device_lib\n",
    "  import os\n",
    "  import sys\n",
    "\n",
    "  import logging\n",
    "  logging.warning('Start custom execution')\n",
    "\n",
    "  try:\n",
    "    tfds.disable_progress_bar()\n",
    "  except Exception as e:\n",
    "    print(f'Exception: {e}')\n",
    "\n",
    "  print('Python Version = {}'.format(sys.version))\n",
    "  print('TensorFlow Version = {}'.format(tf.__version__))\n",
    "  print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "  print('DEVICES', device_lib.list_local_devices())\n",
    "\n",
    "  # Single Machine, single compute device\n",
    "  if distribute == 'single':\n",
    "      if tf.test.is_gpu_available():\n",
    "          strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "      else:\n",
    "          strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "  # Single Machine, multiple compute device\n",
    "  elif args.distribute == 'mirror':\n",
    "      strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "  # Preparing dataset\n",
    "  BUFFER_SIZE = 10000\n",
    "  BATCH_SIZE = 64\n",
    "\n",
    "  def make_datasets_unbatched():\n",
    "    # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n",
    "    def scale(image, label):\n",
    "      image = tf.cast(image, tf.float32)\n",
    "      image /= 255.0\n",
    "      return image, label\n",
    "\n",
    "    print('Make datasets unbatched')\n",
    "    datasets, info = tfds.load(name='cifar10',\n",
    "                              with_info=True,\n",
    "                              as_supervised=True)\n",
    "    return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "\n",
    "  # Build the Keras model\n",
    "  def build_and_compile_cnn_model():\n",
    "    print('Build model')\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "  # Train the model\n",
    "  train_dataset = make_datasets_unbatched().batch(BATCH_SIZE)\n",
    "\n",
    "  with strategy.scope():\n",
    "    # Creation of dataset, and model building/compiling need to be within\n",
    "    # `strategy.scope()`.\n",
    "    model = build_and_compile_cnn_model()\n",
    "    \n",
    "  print('Train model')\n",
    "  model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=steps)\n",
    "  print('Save model')\n",
    "  model.save(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "df7dbc28",
   "metadata": {
    "id": "hEdIFd6eJnuD"
   },
   "outputs": [],
   "source": [
    "# train(lr=0.01, epochs=1, steps=STEPS, distribute='single', model_uri=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17eaab14",
   "metadata": {
    "id": "Zomy-1XUvqFs"
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"train-endpoint-deploy\" + str(uuid.uuid4()))\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    model_display_name: str = MODEL_DISPLAY_NAME,\n",
    "    serving_container_image_uri: str = DEPLOY_IMAGE,\n",
    "    lr: float = 0.01,\n",
    "    epochs: int = EPOCHS,\n",
    "    steps: int = STEPS,\n",
    "    distribute: str = 'single'\n",
    "):\n",
    "\n",
    "    train_task = train(lr=lr, epochs=epochs, steps=steps, distribute=distribute, model_uri=MODEL_DIR)\n",
    "    \n",
    "    experimental.run_as_aiplatform_custom_job(\n",
    "    train_task,\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=TRAIN_GPU,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    )\n",
    "    \n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=MODEL_DIR,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_environment_variables={\"NOT_USED\": \"NO_VALUE\"},\n",
    "    )\n",
    "    model_upload_op.after(train_task)\n",
    "\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"pipelines-created-endpoint\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "        project=project,\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_display_name,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        accelerator_type=DEPLOY_GPU.name,\n",
    "        accelerator_count=DEPLOY_NGPU\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e148a8bf",
   "metadata": {
    "id": "wPVUPLSfvqFt"
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"train_upload_deploy.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef45d77",
   "metadata": {
    "id": "tnR8czclvqFt"
   },
   "source": [
    "The pipeline compilation generates the `image_classif_pipeline.json` job spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec4b8b",
   "metadata": {
    "id": "BNfNNZ6XvqFu"
   },
   "source": [
    "Then, you run the defined pipeline like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1f2c99ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "mN1VPFuRvqFu",
    "outputId": "879307df-6cc1-4726-9cba-1f92719ee836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://auv_vertex_pipeline/pipeline_root/auv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-endpoint-deployc7bedd60-66c1-4777-a87f-3497805777c0-20210524152129?project=google.com:ml-baguette-demos\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(PIPELINE_ROOT)\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    \"train_upload_deploy.json\", pipeline_root=PIPELINE_ROOT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbd59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93e90a10",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## Make an online prediction request\n",
    "\n",
    "Send an online prediction request to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79682857",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "### Get test data\n",
    "\n",
    "Download images from the CIFAR dataset and preprocess them.\n",
    "\n",
    "#### Download the test images\n",
    "\n",
    "Download the provided set of images from the CIFAR dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aba5dd03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1EQBPGnlugz",
    "outputId": "1980a9d4-6a16-4e4b-dc56-3cac6a2fba3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_0_3.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_0_6.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_2_1.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_2_5.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_3_7.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_4_10.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_2_8.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_7_2.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_2_9.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_5_4.jpg...\n",
      "/ [10/10 files][  8.7 KiB/  8.7 KiB] 100% Done                                  \n",
      "Operation completed over 10 objects/8.7 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Download the images\n",
    "! gsutil -m cp -r gs://cloud-samples-data/ai-platform-unified/cifar_test_images ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4d86c",
   "metadata": {
    "id": "prepare_test_item:test,image"
   },
   "source": [
    "#### Preprocess the images\n",
    "Before you can run the data through the endpoint, you need to preprocess it to match the format that your custom model defined in `task.py` expects.\n",
    "\n",
    "`x_test`:\n",
    "Normalize (rescale) the pixel data by dividing each pixel by 255. This replaces each single byte integer pixel with a 32-bit floating point number between 0 and 1.\n",
    "\n",
    "`y_test`:\n",
    "You can extract the labels from the image filenames. Each image's filename format is \"image_{LABEL}_{IMAGE_NUMBER}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3ed5880",
   "metadata": {
    "id": "cl59KGnXlugz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load image data\n",
    "IMAGE_DIRECTORY = \"cifar_test_images\"\n",
    "\n",
    "image_files = [file for file in os.listdir(IMAGE_DIRECTORY) if file.endswith(\".jpg\")]\n",
    "\n",
    "# Decode JPEG images into numpy arrays\n",
    "image_data = [\n",
    "    np.asarray(Image.open(os.path.join(IMAGE_DIRECTORY, file))) for file in image_files\n",
    "]\n",
    "\n",
    "# Scale and convert to expected format\n",
    "x_test = [(image / 255.0).astype(np.float32).tolist() for image in image_data]\n",
    "\n",
    "# Extract labels from image name\n",
    "y_test = [int(file.split(\"_\")[1]) for file in image_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de9df8",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### Send the prediction request\n",
    "\n",
    "Now that you have test images, you can use them to send a prediction request. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n",
    "\n",
    "- `instances`: A list of image instances. According to your custom model, each image instance should be a 3-dimensional matrix of floats. This was prepared in the previous step.\n",
    "\n",
    "The `predict` function returns a list, where each element in the list corresponds to the corresponding image in the request. You will see in the output for each prediction:\n",
    "\n",
    "- Confidence level for the prediction (`predictions`), between 0 and 1, for each of the ten classes.\n",
    "\n",
    "You can then run a quick evaluation on the prediction results:\n",
    "1. `np.argmax`: Convert each list of confidence levels to a label\n",
    "2. Compare the predicted labels to the actual labels\n",
    "3. Calculate `accuracy` as `correct/total`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bc59e22a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UywuX7fRlugz",
    "outputId": "1bc5c946-61b0-4008-f579-049b1655eedb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions = 3, Total predictions = 10, Accuracy = 0.3\n"
     ]
    }
   ],
   "source": [
    "# Get and update the endpoint definiton from the pipeline UI logs:\n",
    "endpoint = aiplatform.Endpoint('projects/660199673046/locations/us-central1/endpoints/1089923886381793280')\n",
    "\n",
    "\n",
    "predictions = endpoint.predict(instances=x_test)\n",
    "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "correct = sum(y_predicted == np.array(y_test))\n",
    "accuracy = len(y_predicted)\n",
    "print(\n",
    "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f0912",
   "metadata": {
    "id": "s4jxmfyT26gj"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "You can delete the individual resources you created in this tutorial:\n",
    "- Delete Cloud Storage objects that were created.  Uncomment and run the command in the cell below **only if you are not using the `PIPELINE_ROOT` path for any other purpose**.\n",
    "- Delete your deployed model: first, undeploy it from its *endpoint*, then delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b07971",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtZCXIi1aULZ",
    "outputId": "88021e71-5ff3-481c-8c58-7ddbff061553"
   },
   "outputs": [],
   "source": [
    "# Warning: this command will delete ALL Cloud Storage objects under the PIPELINE_ROOT path.\n",
    "# ! gsutil -m rm -r $PIPELINE_ROOT"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
