{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6da72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of the Vertex AI Pipeline, serverless ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575bfc0",
   "metadata": {
    "id": "mVX0RV_s8zZ7"
   },
   "source": [
    "# Vertex Pipelines: Model train, upload, and deploy using google-cloud-pipeline-components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113cf15",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b8257",
   "metadata": {
    "id": "IaYsrh0Tc17L"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = \"--user\"\n",
    "else:\n",
    "    USER_FLAG = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a361895",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yxtzwPPNZ-SH",
    "outputId": "92d616ad-4586-4a88-ee15-dab67a2eb152"
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade google-cloud-aiplatform $USER_FLAG\n",
    "!pip3 install --upgrade google-cloud-storage $USER_FLAG\n",
    "!pip3 install --upgrade kfp $USER_FLAG\n",
    "!pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1daa3",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2339288",
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b9582",
   "metadata": {
    "id": "6GPgNN7eeX1l"
   },
   "source": [
    "Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6d77f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NN0mULkEeb84",
    "outputId": "865fafbd-91da-42f7-f4f7-b1ec1e916493"
   },
   "outputs": [],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6cc26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM1iC_MfAts1",
    "outputId": "58256a2d-2fb0-4b30-d450-074d25e09b9d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31297c93",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16997f",
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"google.com:ml-baguette-demos\"  \n",
    "    !gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c4ffd-ec1e-44fa-b00b-167ae72523b5",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce3cf6-5a89-4e2c-8426-c0077477eafe",
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6e47b-9d65-4049-86c8-0342c7665899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMESTAMP = \"20220404161754\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a0a24",
   "metadata": {
    "id": "NxhCPW6e46EF"
   },
   "source": [
    "### Create a Cloud Storage bucket as necessary\n",
    "\n",
    "You will need a Cloud Storage bucket for this example.  If you don't have one that you want to use, you can make one now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ef907-0082-4048-a75b-49305657b3da",
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995d4b1-4e62-4b50-8ef5-2f6bb54f57e8",
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_NAME = BUCKET_NAME.replace('google.com:', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815bdb7-31c6-4a62-b7c3-2db1bbab69a9",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b7bb6-db25-402c-978f-98140dab155a",
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8c54e-40f3-4c38-90e8-f61172d1c24c",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b96a80-af33-499f-b31d-922cacc7236f",
   "metadata": {
    "id": "validate_bucket",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076626ce-68e9-484a-8c3c-f0f6c6c7fad2",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaddc89-2ab8-4b72-b79c-48c29c235878",
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f77d9-135f-4952-86dd-0ca330ca4ed8",
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].replace('*', '').strip()\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5755886-9abc-4035-9f29-8322e50e1c97",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68fe46-abe1-454d-8f66-7376900a38a2",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495ebbb",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1e060",
   "metadata": {
    "id": "YYtGjGG45ELJ"
   },
   "source": [
    "Define some constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28fa32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "5zmD19ryCre7",
    "outputId": "428e4a28-ee49-4d33-943a-92b97b792257"
   },
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/cifar_image_clf\".format(BUCKET_NAME)\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e424a2",
   "metadata": {
    "id": "IprQaSI25oSk"
   },
   "source": [
    "Do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb016d-ee9a-459a-a489-3916c9d26687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.components import importer_node\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.custom_job import \\\n",
    "    CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
    "                                                          ModelDeployOp)\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9e7e8",
   "metadata": {
    "id": "accelerators:training,prediction"
   },
   "source": [
    "#### Set hardware accelerators\n",
    "You can set hardware accelerators for both training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba731b",
   "metadata": {
    "id": "xd5PLXDTlugv"
   },
   "outputs": [],
   "source": [
    "TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
    "\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da548bf",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "#### Set pre-built containers\n",
    "\n",
    "Vertex AI provides pre-built containers to run training and prediction.\n",
    "\n",
    "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers) and [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32baa577",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1u1mr18jlugv",
    "outputId": "6ebe2573-fe13-4e2c-9cfc-31e66c3514be"
   },
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"tf-gpu.2-4\"\n",
    "DEPLOY_VERSION = \"tf2-gpu.2-4\" \n",
    "\n",
    "TF2_GPU_IMAGE = \"tensorflow/tensorflow:latest-gpu\"\n",
    "TRAIN_IMAGE = \"gcr.io/cloud-aiplatform/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TF2_GPU_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d14c8",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### Set machine types\n",
    "\n",
    "Next, set the machine types to use for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03369e08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAXwbqKKlugv",
    "outputId": "2110ed52-8458-4d2e-9926-383474b01393"
   },
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdd100",
   "metadata": {
    "id": "AqoS1fRn76ff",
    "tags": []
   },
   "source": [
    "#### Instantiate an Vertex Pipeline KFP API client object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4065a6-95b1-4433-a263-914462f7227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929396bc",
   "metadata": {
    "id": "train_custom_model"
   },
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e45bf9",
   "metadata": {
    "id": "1npiDcUtlugw"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_\" + TIMESTAMP\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_NAME, JOB_NAME)\n",
    "MODEL_DISPLAY_NAME = \"cifar10-\" + TIMESTAMP\n",
    "\n",
    "if not TRAIN_NGPU or TRAIN_NGPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"mirror\"\n",
    "\n",
    "EPOCHS = 10\n",
    "STEPS = 100\n",
    "\n",
    "print(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f52d0c",
   "metadata": {
    "id": "taskpy_contents"
   },
   "source": [
    "#### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da836229",
   "metadata": {
    "id": "NJK00XwJuDJD"
   },
   "outputs": [],
   "source": [
    "# Prepare a function to convert it to a component\n",
    "\n",
    "@component(base_image=TF2_GPU_IMAGE,\n",
    "           output_component_file='train_op.yaml', \n",
    "           packages_to_install=['tensorflow-datasets'])\n",
    "def train(\n",
    "  lr: float,\n",
    "  epochs: int,\n",
    "  steps: int,\n",
    "  distribute: str,\n",
    "  model_uri: str,\n",
    "):\n",
    "  \n",
    "  # Imports\n",
    "  import tensorflow_datasets as tfds\n",
    "  import tensorflow as tf\n",
    "  from tensorflow.python.client import device_lib\n",
    "  import os\n",
    "  import sys\n",
    "\n",
    "  import logging\n",
    "  logging.warning('Start custom execution')\n",
    "\n",
    "  try:\n",
    "    tfds.disable_progress_bar()\n",
    "  except Exception as e:\n",
    "    print(f'Exception: {e}')\n",
    "\n",
    "  print('Python Version = {}'.format(sys.version))\n",
    "  print('TensorFlow Version = {}'.format(tf.__version__))\n",
    "  print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "  print('DEVICES', device_lib.list_local_devices())\n",
    "\n",
    "  # Single Machine, single compute device\n",
    "  if distribute == 'single':\n",
    "      if tf.test.is_gpu_available():\n",
    "          strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "      else:\n",
    "          strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "  # Single Machine, multiple compute device\n",
    "  elif args.distribute == 'mirror':\n",
    "      strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "  # Preparing dataset\n",
    "  BUFFER_SIZE = 10000\n",
    "  BATCH_SIZE = 64\n",
    "\n",
    "  def make_datasets_unbatched():\n",
    "    # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n",
    "    def scale(image, label):\n",
    "      image = tf.cast(image, tf.float32)\n",
    "      image /= 255.0\n",
    "      return image, label\n",
    "\n",
    "    print('Make datasets unbatched')\n",
    "    datasets, info = tfds.load(name='cifar10',\n",
    "                              with_info=True,\n",
    "                              as_supervised=True)\n",
    "    return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "\n",
    "  # Build the Keras model\n",
    "  def build_and_compile_cnn_model():\n",
    "    print('Build model')\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "  # Train the model\n",
    "  train_dataset = make_datasets_unbatched().batch(BATCH_SIZE)\n",
    "\n",
    "  with strategy.scope():\n",
    "    # Creation of dataset, and model building/compiling need to be within\n",
    "    # `strategy.scope()`.\n",
    "    model = build_and_compile_cnn_model()\n",
    "    \n",
    "  print('Train model')\n",
    "  model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=steps)\n",
    "  print('Save model')\n",
    "  model.save(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6c355",
   "metadata": {
    "id": "hEdIFd6eJnuD"
   },
   "outputs": [],
   "source": [
    "# train(lr=0.01, epochs=1, steps=STEPS, distribute='single', model_uri=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d90c4c-1245-4cc8-a0d2-724c31c37044",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_job_distributed_training_op = utils.create_custom_training_job_op_from_component(\n",
    "    train,\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=TRAIN_GPU,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb7706",
   "metadata": {
    "id": "Zomy-1XUvqFs"
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"train-endpoint-deploy\" + str(uuid.uuid4()))\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    model_display_name: str = MODEL_DISPLAY_NAME,\n",
    "    lr: float = 0.01,\n",
    "    epochs: int = EPOCHS,\n",
    "    steps: int = STEPS,\n",
    "    distribute: str = 'single'\n",
    "):\n",
    "\n",
    "\n",
    "    custom_producer_task = custom_job_distributed_training_op(\n",
    "        model_uri=MODEL_DIR,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        steps=steps,\n",
    "        distribute=distribute,\n",
    "        project=project,\n",
    "        location=REGION,\n",
    "        base_output_directory=PIPELINE_ROOT,\n",
    "    )\n",
    "\n",
    "    import_unmanaged_model_task = importer_node.importer(\n",
    "        artifact_uri=MODEL_DIR,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": DEPLOY_IMAGE,\n",
    "            },\n",
    "        },\n",
    "    ).after(custom_producer_task)\n",
    "    \n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=model_display_name,\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    )\n",
    "    model_upload_op.after(import_unmanaged_model_task)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"pipelines-created-endpoint\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=model_display_name,\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_accelerator_type=DEPLOY_GPU.name,\n",
    "        dedicated_resources_accelerator_count=DEPLOY_NGPU,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f68287",
   "metadata": {
    "id": "wPVUPLSfvqFt"
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"train_upload_deploy.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca65832",
   "metadata": {
    "id": "tnR8czclvqFt"
   },
   "source": [
    "The pipeline compilation generates the `image_classif_pipeline.json` job spec file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a334b4f",
   "metadata": {
    "id": "BNfNNZ6XvqFu"
   },
   "source": [
    "Then, you run the defined pipeline like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = MODEL_DISPLAY_NAME + \"-train-pipeline\"\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"train_upload_deploy.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6de1c",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## Make an online prediction request\n",
    "\n",
    "Send an online prediction request to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb72f7",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "### Get test data\n",
    "\n",
    "Download images from the CIFAR dataset and preprocess them.\n",
    "\n",
    "#### Download the test images\n",
    "\n",
    "Download the provided set of images from the CIFAR dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841130e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1EQBPGnlugz",
    "outputId": "1980a9d4-6a16-4e4b-dc56-3cac6a2fba3d"
   },
   "outputs": [],
   "source": [
    "# Download the images\n",
    "! gsutil -m cp -r gs://cloud-samples-data/ai-platform-unified/cifar_test_images ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dda3b2",
   "metadata": {
    "id": "prepare_test_item:test,image"
   },
   "source": [
    "#### Preprocess the images\n",
    "Before you can run the data through the endpoint, you need to preprocess it to match the format that your custom model defined in `task.py` expects.\n",
    "\n",
    "`x_test`:\n",
    "Normalize (rescale) the pixel data by dividing each pixel by 255. This replaces each single byte integer pixel with a 32-bit floating point number between 0 and 1.\n",
    "\n",
    "`y_test`:\n",
    "You can extract the labels from the image filenames. Each image's filename format is \"image_{LABEL}_{IMAGE_NUMBER}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ca144",
   "metadata": {
    "id": "cl59KGnXlugz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load image data\n",
    "IMAGE_DIRECTORY = \"cifar_test_images\"\n",
    "\n",
    "image_files = [file for file in os.listdir(IMAGE_DIRECTORY) if file.endswith(\".jpg\")]\n",
    "\n",
    "# Decode JPEG images into numpy arrays\n",
    "image_data = [\n",
    "    np.asarray(Image.open(os.path.join(IMAGE_DIRECTORY, file))) for file in image_files\n",
    "]\n",
    "\n",
    "# Scale and convert to expected format\n",
    "x_test = [(image / 255.0).astype(np.float32).tolist() for image in image_data]\n",
    "\n",
    "# Extract labels from image name\n",
    "y_test = [int(file.split(\"_\")[1]) for file in image_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d9895",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### Send the prediction request\n",
    "\n",
    "Now that you have test images, you can use them to send a prediction request. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n",
    "\n",
    "- `instances`: A list of image instances. According to your custom model, each image instance should be a 3-dimensional matrix of floats. This was prepared in the previous step.\n",
    "\n",
    "The `predict` function returns a list, where each element in the list corresponds to the corresponding image in the request. You will see in the output for each prediction:\n",
    "\n",
    "- Confidence level for the prediction (`predictions`), between 0 and 1, for each of the ten classes.\n",
    "\n",
    "You can then run a quick evaluation on the prediction results:\n",
    "1. `np.argmax`: Convert each list of confidence levels to a label\n",
    "2. Compare the predicted labels to the actual labels\n",
    "3. Calculate `accuracy` as `correct/total`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17bdcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UywuX7fRlugz",
    "outputId": "1bc5c946-61b0-4008-f579-049b1655eedb"
   },
   "outputs": [],
   "source": [
    "# Get and update the endpoint definiton from the pipeline UI logs:\n",
    "endpoint_resource_name = \"projects/660199673046/locations/us-central1/endpoints/3099153842793611264\"\n",
    "endpoint = aip.Endpoint(endpoint_resource_name)\n",
    "\n",
    "\n",
    "predictions = endpoint.predict(instances=x_test)\n",
    "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "correct = sum(y_predicted == np.array(y_test))\n",
    "accuracy = len(y_predicted)\n",
    "print(\n",
    "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14e82f",
   "metadata": {
    "id": "s4jxmfyT26gj"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "You can delete the individual resources you created in this tutorial:\n",
    "- Delete Cloud Storage objects that were created.  Uncomment and run the command in the cell below **only if you are not using the `PIPELINE_ROOT` path for any other purpose**.\n",
    "- Delete your deployed model: first, undeploy it from its *endpoint*, then delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13503dcb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtZCXIi1aULZ",
    "outputId": "88021e71-5ff3-481c-8c58-7ddbff061553"
   },
   "outputs": [],
   "source": [
    "# Warning: this command will delete ALL Cloud Storage objects under the PIPELINE_ROOT path.\n",
    "# ! gsutil -m rm -r $PIPELINE_ROOT"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
